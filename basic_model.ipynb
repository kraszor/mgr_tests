{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4vzN4PdhOFsW9U562o3of",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kraszor/mgr_tests/blob/main/basic_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LeGA9_rupTmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import glob\n",
        "import random\n",
        "\n",
        "class ParquetDataset(IterableDataset):\n",
        "    def __init__(self, folder_path, features, target, batch_size=1024, shuffle_files=True):\n",
        "        self.folder_path = folder_path\n",
        "        self.files = glob.glob(f\"{folder_path}/*.parquet\")\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle_files = shuffle_files\n",
        "\n",
        "    def __iter__(self):\n",
        "        files = self.files.copy()\n",
        "        if self.shuffle_files:\n",
        "            random.shuffle(files)\n",
        "\n",
        "        for file_path in files:\n",
        "            parquet_file = pq.ParquetFile(file_path)\n",
        "            for batch in parquet_file.iter_batches(batch_size=self.batch_size, columns=self.features + [self.target]):\n",
        "                batch = batch.to_pydict()\n",
        "                x = torch.tensor(\n",
        "                    [[batch[f][i] for f in self.features] for i in range(len(batch[self.target]))],\n",
        "                    dtype=torch.float32\n",
        "                )\n",
        "                y = torch.tensor(batch[self.target], dtype=torch.float32)\n",
        "                yield x, y"
      ],
      "metadata": {
        "id": "YxeQ_tjLpg7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=4112, hidden_dims=[512, 256, 128]):\n",
        "        \"\"\"\n",
        "        Simple feedforward neural network for binary classification.\n",
        "\n",
        "        Args:\n",
        "            input_dim: Total input dimension (2056 + 2056 = 4112)\n",
        "            hidden_dims: List of hidden layer dimensions\n",
        "        \"\"\"\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(loader):\n",
        "        x = x.view(x.size(0), -1).to(device)\n",
        "        y = y.unsqueeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        correct += (predictions == y).sum().item()\n",
        "        total += y.size(0)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}, Loss: {loss.item():.4f}, \"\n",
        "                  f\"Acc: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    avg_loss = total_loss / (batch_idx + 1)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "def train_model(train_loader, val_loader=None, epochs=10, lr=0.001, device='cuda'):\n",
        "    \"\"\"\n",
        "    Main training function.\n",
        "\n",
        "    Args:\n",
        "        train_loader: DataLoader for training data\n",
        "        val_loader: DataLoader for validation data (optional)\n",
        "        epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        device: 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = BinaryClassifier(input_dim=4112).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        if val_loader is not None:\n",
        "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), 'best_model.pth')\n",
        "                print(\"âœ“ Saved best model\")\n",
        "        else:\n",
        "            scheduler.step(train_loss)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JKzx8vV3zG9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # Create datasets\n",
        "#     features = ['POS_vector', 'Patho_Vector']\n",
        "#     target = 'ClinSigSimple'\n",
        "\n",
        "#     train_dataset = ParquetDataset(\n",
        "#         folder_path=\"data.parquet\",\n",
        "#         features=features,\n",
        "#         target=target,\n",
        "#         batch_size=256,\n",
        "#         shuffle_files=True\n",
        "#     )\n",
        "\n",
        "#     # val_dataset = ParquetDataset(\n",
        "#     #     folder_path=\"data.parquet/val\",\n",
        "#     #     features=features,\n",
        "#     #     target=target,\n",
        "#     #     batch_size=32,\n",
        "#     #     shuffle_files=False\n",
        "#     # )\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=None, num_workers=0)\n",
        "#     # val_loader = DataLoader(val_dataset, batch_size=None, num_workers=0)\n",
        "\n",
        "#     # Train model\n",
        "#     model = train_model(\n",
        "#         train_loader=train_loader,\n",
        "#         # val_loader=val_loader,\n",
        "#         epochs=10,\n",
        "#         lr=0.001,\n",
        "#         device='cpu'\n",
        "#     )\n",
        "\n",
        "    # # Load best model for inference\n",
        "    # model.load_state_dict(torch.load('best_model.pth'))\n",
        "    # model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIa5bcE3pMoL",
        "outputId": "655c2058-e39a-4434-f2f8-b84b7de873b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Epoch 1/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.6391, Acc: 63.79%\n",
            "  Batch 20, Loss: 0.5278, Acc: 66.70%\n",
            "  Batch 30, Loss: 0.6045, Acc: 69.11%\n",
            "  Batch 40, Loss: 0.4937, Acc: 70.20%\n",
            "  Batch 50, Loss: 0.4574, Acc: 71.28%\n",
            "  Batch 60, Loss: 0.5453, Acc: 71.75%\n",
            "  Batch 70, Loss: 0.5270, Acc: 72.24%\n",
            "  Batch 80, Loss: 0.4755, Acc: 72.69%\n",
            "  Batch 90, Loss: 0.5213, Acc: 72.92%\n",
            "  Batch 100, Loss: 0.5458, Acc: 73.06%\n",
            "  Batch 110, Loss: 0.4956, Acc: 73.31%\n",
            "  Batch 120, Loss: 0.5041, Acc: 73.60%\n",
            "  Batch 130, Loss: 0.5054, Acc: 73.86%\n",
            "  Batch 140, Loss: 0.4440, Acc: 74.03%\n",
            "  Batch 150, Loss: 0.5092, Acc: 74.13%\n",
            "  Batch 160, Loss: 0.4849, Acc: 74.30%\n",
            "  Batch 170, Loss: 0.4707, Acc: 74.36%\n",
            "  Batch 180, Loss: 0.4959, Acc: 74.47%\n",
            "  Batch 190, Loss: 0.4861, Acc: 74.60%\n",
            "Train Loss: 0.5200, Train Acc: 74.62%\n",
            "\n",
            "Epoch 2/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.4221, Acc: 81.13%\n",
            "  Batch 20, Loss: 0.4107, Acc: 81.39%\n",
            "  Batch 30, Loss: 0.4520, Acc: 81.09%\n",
            "  Batch 40, Loss: 0.3984, Acc: 80.90%\n",
            "  Batch 50, Loss: 0.5328, Acc: 80.62%\n",
            "  Batch 60, Loss: 0.4683, Acc: 79.84%\n",
            "  Batch 70, Loss: 0.4752, Acc: 79.28%\n",
            "  Batch 80, Loss: 0.4881, Acc: 79.21%\n",
            "  Batch 90, Loss: 0.4314, Acc: 79.01%\n",
            "  Batch 100, Loss: 0.4946, Acc: 79.00%\n",
            "  Batch 110, Loss: 0.4774, Acc: 78.94%\n",
            "  Batch 120, Loss: 0.4558, Acc: 79.01%\n",
            "  Batch 130, Loss: 0.4613, Acc: 79.08%\n",
            "  Batch 140, Loss: 0.4565, Acc: 79.15%\n",
            "  Batch 150, Loss: 0.5170, Acc: 79.12%\n",
            "  Batch 160, Loss: 0.4664, Acc: 79.01%\n",
            "  Batch 170, Loss: 0.4749, Acc: 78.94%\n",
            "  Batch 180, Loss: 0.4417, Acc: 78.93%\n",
            "  Batch 190, Loss: 0.4296, Acc: 78.84%\n",
            "Train Loss: 0.4524, Train Acc: 78.85%\n",
            "\n",
            "Epoch 3/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.4316, Acc: 80.74%\n",
            "  Batch 20, Loss: 0.3857, Acc: 80.94%\n",
            "  Batch 30, Loss: 0.4073, Acc: 81.16%\n",
            "  Batch 40, Loss: 0.4515, Acc: 81.29%\n",
            "  Batch 50, Loss: 0.5094, Acc: 81.30%\n",
            "  Batch 60, Loss: 0.3926, Acc: 81.08%\n",
            "  Batch 70, Loss: 0.4117, Acc: 80.91%\n",
            "  Batch 80, Loss: 0.4038, Acc: 80.97%\n",
            "  Batch 90, Loss: 0.3688, Acc: 81.20%\n",
            "  Batch 100, Loss: 0.4508, Acc: 81.24%\n",
            "  Batch 110, Loss: 0.4754, Acc: 80.91%\n",
            "  Batch 120, Loss: 0.4233, Acc: 80.75%\n",
            "  Batch 130, Loss: 0.3905, Acc: 80.74%\n",
            "  Batch 140, Loss: 0.4089, Acc: 80.70%\n",
            "  Batch 150, Loss: 0.4575, Acc: 80.72%\n",
            "  Batch 160, Loss: 0.4813, Acc: 80.52%\n",
            "  Batch 170, Loss: 0.4140, Acc: 80.41%\n",
            "  Batch 180, Loss: 0.4810, Acc: 80.36%\n",
            "  Batch 190, Loss: 0.4079, Acc: 80.39%\n",
            "Train Loss: 0.4207, Train Acc: 80.44%\n",
            "\n",
            "Epoch 4/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.4560, Acc: 80.04%\n",
            "  Batch 20, Loss: 0.4112, Acc: 80.78%\n",
            "  Batch 30, Loss: 0.4578, Acc: 81.12%\n",
            "  Batch 40, Loss: 0.3554, Acc: 81.62%\n",
            "  Batch 50, Loss: 0.1909, Acc: 82.07%\n",
            "  Batch 60, Loss: 0.3992, Acc: 81.92%\n",
            "  Batch 70, Loss: 0.3616, Acc: 81.84%\n",
            "  Batch 80, Loss: 0.4483, Acc: 81.79%\n",
            "  Batch 90, Loss: 0.3950, Acc: 81.80%\n",
            "  Batch 100, Loss: 0.4675, Acc: 81.92%\n",
            "  Batch 110, Loss: 0.4337, Acc: 81.73%\n",
            "  Batch 120, Loss: 0.4172, Acc: 81.66%\n",
            "  Batch 130, Loss: 0.3874, Acc: 81.72%\n",
            "  Batch 140, Loss: 0.3579, Acc: 81.70%\n",
            "  Batch 150, Loss: 0.4108, Acc: 81.72%\n",
            "  Batch 160, Loss: 0.3997, Acc: 81.64%\n",
            "  Batch 170, Loss: 0.3991, Acc: 81.64%\n",
            "  Batch 180, Loss: 0.4031, Acc: 81.63%\n",
            "  Batch 190, Loss: 0.4085, Acc: 81.64%\n",
            "Train Loss: 0.3962, Train Acc: 81.61%\n",
            "\n",
            "Epoch 5/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.3392, Acc: 83.32%\n",
            "  Batch 20, Loss: 0.3485, Acc: 83.95%\n",
            "  Batch 30, Loss: 0.3406, Acc: 84.61%\n",
            "  Batch 40, Loss: 0.3022, Acc: 84.65%\n",
            "  Batch 50, Loss: 0.3236, Acc: 84.54%\n",
            "  Batch 60, Loss: 0.4659, Acc: 83.75%\n",
            "  Batch 70, Loss: 0.4108, Acc: 83.21%\n",
            "  Batch 80, Loss: 0.3374, Acc: 83.03%\n",
            "  Batch 90, Loss: 0.4118, Acc: 82.96%\n",
            "  Batch 100, Loss: 0.4359, Acc: 83.02%\n",
            "  Batch 110, Loss: 0.3662, Acc: 83.07%\n",
            "  Batch 120, Loss: 0.3892, Acc: 83.00%\n",
            "  Batch 130, Loss: 0.3987, Acc: 82.97%\n",
            "  Batch 140, Loss: 0.3617, Acc: 83.00%\n",
            "  Batch 150, Loss: 0.4269, Acc: 83.06%\n",
            "  Batch 160, Loss: 0.4013, Acc: 83.02%\n",
            "  Batch 170, Loss: 0.4184, Acc: 82.98%\n",
            "  Batch 180, Loss: 0.3696, Acc: 83.02%\n",
            "  Batch 190, Loss: 0.3711, Acc: 83.10%\n",
            "Train Loss: 0.3683, Train Acc: 83.10%\n",
            "\n",
            "Epoch 6/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.3794, Acc: 82.30%\n",
            "  Batch 20, Loss: 0.3052, Acc: 82.70%\n",
            "  Batch 30, Loss: 0.4061, Acc: 83.35%\n",
            "  Batch 40, Loss: 0.3375, Acc: 83.60%\n",
            "  Batch 50, Loss: 0.3019, Acc: 84.13%\n",
            "  Batch 60, Loss: 0.4100, Acc: 83.66%\n",
            "  Batch 70, Loss: 0.4061, Acc: 83.44%\n",
            "  Batch 80, Loss: 0.3416, Acc: 83.48%\n",
            "  Batch 90, Loss: 0.3760, Acc: 83.54%\n",
            "  Batch 100, Loss: 0.4224, Acc: 83.54%\n",
            "  Batch 110, Loss: 0.4144, Acc: 83.39%\n",
            "  Batch 120, Loss: 0.3272, Acc: 83.52%\n",
            "  Batch 130, Loss: 0.3028, Acc: 83.61%\n",
            "  Batch 140, Loss: 0.3268, Acc: 83.75%\n",
            "  Batch 150, Loss: 0.4469, Acc: 83.80%\n",
            "  Batch 160, Loss: 0.3794, Acc: 83.68%\n",
            "  Batch 170, Loss: 0.3983, Acc: 83.60%\n",
            "  Batch 180, Loss: 0.3708, Acc: 83.63%\n",
            "  Batch 190, Loss: 0.3819, Acc: 83.61%\n",
            "Train Loss: 0.3587, Train Acc: 83.64%\n",
            "\n",
            "Epoch 7/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.4298, Acc: 82.66%\n",
            "  Batch 20, Loss: 0.3899, Acc: 83.09%\n",
            "  Batch 30, Loss: 0.4146, Acc: 83.50%\n",
            "  Batch 40, Loss: 0.3439, Acc: 83.91%\n",
            "  Batch 50, Loss: 0.1730, Acc: 84.40%\n",
            "  Batch 60, Loss: 0.3412, Acc: 84.42%\n",
            "  Batch 70, Loss: 0.3089, Acc: 84.46%\n",
            "  Batch 80, Loss: 0.3652, Acc: 84.50%\n",
            "  Batch 90, Loss: 0.2848, Acc: 84.67%\n",
            "  Batch 100, Loss: 0.3260, Acc: 84.94%\n",
            "  Batch 110, Loss: 0.3433, Acc: 84.89%\n",
            "  Batch 120, Loss: 0.3149, Acc: 84.91%\n",
            "  Batch 130, Loss: 0.3310, Acc: 84.91%\n",
            "  Batch 140, Loss: 0.2619, Acc: 85.02%\n",
            "  Batch 150, Loss: 0.3534, Acc: 85.07%\n",
            "  Batch 160, Loss: 0.3532, Acc: 84.99%\n",
            "  Batch 170, Loss: 0.3269, Acc: 84.98%\n",
            "  Batch 180, Loss: 0.3312, Acc: 85.01%\n",
            "  Batch 190, Loss: 0.2909, Acc: 85.13%\n",
            "Train Loss: 0.3274, Train Acc: 85.23%\n",
            "\n",
            "Epoch 8/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.3889, Acc: 83.98%\n",
            "  Batch 20, Loss: 0.3205, Acc: 84.04%\n",
            "  Batch 30, Loss: 0.3978, Acc: 84.64%\n",
            "  Batch 40, Loss: 0.3065, Acc: 85.03%\n",
            "  Batch 50, Loss: 0.1406, Acc: 85.47%\n",
            "  Batch 60, Loss: 0.2722, Acc: 85.64%\n",
            "  Batch 70, Loss: 0.3126, Acc: 85.56%\n",
            "  Batch 80, Loss: 0.2754, Acc: 85.85%\n",
            "  Batch 90, Loss: 0.2349, Acc: 86.08%\n",
            "  Batch 100, Loss: 0.3455, Acc: 86.16%\n",
            "  Batch 110, Loss: 0.2945, Acc: 85.93%\n",
            "  Batch 120, Loss: 0.3082, Acc: 85.82%\n",
            "  Batch 130, Loss: 0.3160, Acc: 85.77%\n",
            "  Batch 140, Loss: 0.2812, Acc: 85.82%\n",
            "  Batch 150, Loss: 0.3608, Acc: 85.82%\n",
            "  Batch 160, Loss: 0.4232, Acc: 85.65%\n",
            "  Batch 170, Loss: 0.2902, Acc: 85.58%\n",
            "  Batch 180, Loss: 0.3283, Acc: 85.54%\n",
            "  Batch 190, Loss: 0.3010, Acc: 85.63%\n",
            "Train Loss: 0.3162, Train Acc: 85.69%\n",
            "\n",
            "Epoch 9/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.3376, Acc: 87.30%\n",
            "  Batch 20, Loss: 0.2324, Acc: 86.72%\n",
            "  Batch 30, Loss: 0.3223, Acc: 87.25%\n",
            "  Batch 40, Loss: 0.2121, Acc: 87.85%\n",
            "  Batch 50, Loss: 0.3425, Acc: 87.93%\n",
            "  Batch 60, Loss: 0.3143, Acc: 87.36%\n",
            "  Batch 70, Loss: 0.2385, Acc: 87.35%\n",
            "  Batch 80, Loss: 0.2557, Acc: 87.37%\n",
            "  Batch 90, Loss: 0.2507, Acc: 87.38%\n",
            "  Batch 100, Loss: 0.3749, Acc: 87.39%\n",
            "  Batch 110, Loss: 0.3457, Acc: 87.18%\n",
            "  Batch 120, Loss: 0.3520, Acc: 86.98%\n",
            "  Batch 130, Loss: 0.3299, Acc: 86.89%\n",
            "  Batch 140, Loss: 0.2796, Acc: 86.89%\n",
            "  Batch 150, Loss: 0.3666, Acc: 86.88%\n",
            "  Batch 160, Loss: 0.3391, Acc: 86.68%\n",
            "  Batch 170, Loss: 0.3279, Acc: 86.56%\n",
            "  Batch 180, Loss: 0.3153, Acc: 86.51%\n",
            "  Batch 190, Loss: 0.2636, Acc: 86.54%\n",
            "Train Loss: 0.2974, Train Acc: 86.58%\n",
            "\n",
            "Epoch 10/10\n",
            "--------------------------------------------------\n",
            "  Batch 10, Loss: 0.2724, Acc: 86.95%\n",
            "  Batch 20, Loss: 0.2546, Acc: 86.76%\n",
            "  Batch 30, Loss: 0.2995, Acc: 87.03%\n",
            "  Batch 40, Loss: 0.2102, Acc: 87.51%\n",
            "  Batch 50, Loss: 0.2618, Acc: 87.77%\n",
            "  Batch 60, Loss: 0.3397, Acc: 87.37%\n",
            "  Batch 70, Loss: 0.2935, Acc: 87.22%\n",
            "  Batch 80, Loss: 0.2719, Acc: 87.16%\n",
            "  Batch 90, Loss: 0.2799, Acc: 87.26%\n",
            "  Batch 100, Loss: 0.3512, Acc: 87.38%\n",
            "  Batch 110, Loss: 0.3492, Acc: 87.31%\n",
            "  Batch 120, Loss: 0.2560, Acc: 87.27%\n",
            "  Batch 130, Loss: 0.2579, Acc: 87.37%\n",
            "  Batch 140, Loss: 0.2000, Acc: 87.50%\n",
            "  Batch 150, Loss: 0.2844, Acc: 87.61%\n",
            "  Batch 160, Loss: 0.2756, Acc: 87.54%\n",
            "  Batch 170, Loss: 0.3318, Acc: 87.36%\n",
            "  Batch 180, Loss: 0.2856, Acc: 87.29%\n",
            "  Batch 190, Loss: 0.3034, Acc: 87.29%\n",
            "Train Loss: 0.2835, Train Acc: 87.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K2IpAWAe5c02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(loader):\n",
        "            x = x.view(x.size(0), -1).to(device)\n",
        "            y = y.unsqueeze(1).to(device)\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (predictions == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (batch_idx + 1)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate model on any dataset and return detailed metrics.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        loader: DataLoader with data to evaluate\n",
        "        device: 'cuda' or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: Average loss\n",
        "        accuracy: Accuracy percentage\n",
        "    \"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(loader):\n",
        "            x = x.view(x.size(0), -1).to(device)\n",
        "            y = y.unsqueeze(1).to(device)\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (predictions == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print(f\"  Processed {total} samples...\")\n",
        "\n",
        "    avg_loss = total_loss / (batch_idx + 1)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"  Total samples: {total}\")\n",
        "    print(f\"  Correct predictions: {correct}\")\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "0LqHnfbh6Mh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Evaluating on Training Set:\")\n",
        "print(\"=\"*50)\n",
        "train_loss, train_acc = evaluate_model(model, train_loader, device='cpu')\n",
        "print(f\"Training Set - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkWGOlUcyJDf",
        "outputId": "80a91f07-016a-421d-8c6c-131a9ffb7906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Evaluating on Training Set:\n",
            "==================================================\n",
            "Evaluating...\n",
            "  Processed 12711 samples...\n",
            "  Processed 25353 samples...\n",
            "  Processed 37938 samples...\n",
            "  Total samples: 49808\n",
            "  Correct predictions: 42063\n",
            "Training Set - Loss: 0.3717, Accuracy: 84.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['POS_vector', 'Patho_Vector']\n",
        "target = 'ClinSigSimple'\n",
        "val_dataset = ParquetDataset(\n",
        "        folder_path=\"test_data.parquet\",\n",
        "        features=features,\n",
        "        target=target,\n",
        "        batch_size=256,\n",
        "        shuffle_files=False\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=None, num_workers=0)"
      ],
      "metadata": {
        "id": "s4HafL8gyz5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BinaryClassifier(input_dim=4112).to('cpu')\n",
        "model.load_state_dict(torch.load('best_model.pth', map_location='cpu'))\n",
        "\n",
        "val_loss, val_acc = evaluate_model(model, val_loader, device='cpu')\n",
        "print(f\"Validation Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "HjkMN8vM7C8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6235082-30c9-4eca-c7c7-50bc216f20a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n",
            "  Processed 12703 samples...\n",
            "  Total samples: 12703\n",
            "  Correct predictions: 9843\n",
            "Validation Accuracy: 77.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-KXH8aTzjd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}